{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting for un/rhyming poems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting prompts and models\n",
    "\n",
    "Edit `PROMPTS` and `MODEL_LIST` in `constants.py` to change prompts and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from generative_formalism import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### `describe_prompts`\n",
       "\n",
       "```md\n",
       "Print a description of the prompts with statistics and details.\n",
       "    \n",
       "    Args:\n",
       "        prompts: List of prompt strings to describe. Defaults to PROMPT_LIST.\n",
       "        prompt_to_type: Dictionary mapping prompts to their types. Defaults to PROMPT_TO_TYPE.\n",
       "    \n",
       "```\n",
       "----\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documentation(describe_prompts, signature=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 23 unique prompts\n",
      "* 3 prompt types\n",
      "\n",
      "* List of prompts:\n",
      "  ['Write a poem in ballad stanzas.',\n",
      " \"Write an ryhmed poem in the style of Shakespeare's sonnets.\",\n",
      " 'Write a long poem that does rhyme.',\n",
      " 'Write a poem in the style of Emily Dickinson.',\n",
      " 'Write a poem in heroic couplets.',\n",
      " 'Write an rhyming poem.',\n",
      " 'Write a poem (with 20+ lines) that rhymes.',\n",
      " 'Write a poem that does rhyme.',\n",
      " 'Write a short poem that does rhyme.',\n",
      " 'Write a poem that does NOT rhyme.',\n",
      " 'Write a poem (with 20+ lines) that does NOT rhyme.',\n",
      " 'Write a long poem that does NOT rhyme.',\n",
      " 'Write a poem in the style of Walt Whitman.',\n",
      " 'Write a poem in free verse.',\n",
      " 'Write a poem in blank verse.',\n",
      " 'Write an unrhymed poem.',\n",
      " 'Write a short poem that does NOT rhyme.',\n",
      " 'Write a poem (with 20+ lines).',\n",
      " 'Write a long poem.',\n",
      " 'Write a poem in groups of two lines.',\n",
      " 'Write a poem.',\n",
      " 'Write a poem in stanzas of 4 lines each.',\n",
      " 'Write a short poem.']\n",
      "\n",
      "* List of prompt types:\n",
      "  {'DO_rhyme': ['Write a poem in ballad stanzas.',\n",
      "              \"Write an ryhmed poem in the style of Shakespeare's sonnets.\",\n",
      "              'Write a long poem that does rhyme.',\n",
      "              'Write a poem in the style of Emily Dickinson.',\n",
      "              'Write a poem in heroic couplets.',\n",
      "              'Write an rhyming poem.',\n",
      "              'Write a poem (with 20+ lines) that rhymes.',\n",
      "              'Write a poem that does rhyme.',\n",
      "              'Write a short poem that does rhyme.'],\n",
      " 'MAYBE_rhyme': ['Write a poem (with 20+ lines).',\n",
      "                 'Write a long poem.',\n",
      "                 'Write a poem in groups of two lines.',\n",
      "                 'Write a poem.',\n",
      "                 'Write a poem in stanzas of 4 lines each.',\n",
      "                 'Write a short poem.'],\n",
      " 'do_NOT_rhyme': ['Write a poem that does NOT rhyme.',\n",
      "                  'Write a poem (with 20+ lines) that does NOT rhyme.',\n",
      "                  'Write a long poem that does NOT rhyme.',\n",
      "                  'Write a poem in the style of Walt Whitman.',\n",
      "                  'Write a poem in free verse.',\n",
      "                  'Write a poem in blank verse.',\n",
      "                  'Write an unrhymed poem.',\n",
      "                  'Write a short poem that does NOT rhyme.']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describe_prompts(\n",
    "    prompts=PROMPT_LIST,\n",
    "    prompt_to_type=PROMPT_TO_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 11 models (counting parameter changes)\n",
      "  * 6 model types (ChatGPT, Claude, DeepSeek, Gemini, Llama, Olmo)\n",
      "  * Using models:\n",
      "  {   'ChatGPT': ['gpt-3.5-turbo', 'gpt-4-turbo'],\n",
      "    'Claude': [   'claude-3-haiku-20240307',\n",
      "                  'claude-3-opus-20240229',\n",
      "                  'claude-3-sonnet-20240229'],\n",
      "    'DeepSeek': ['deepseek/deepseek-chat'],\n",
      "    'Gemini': ['gemini-pro'],\n",
      "    'Llama': ['ollama/llama3.1:70b', 'ollama/llama3.1:8b'],\n",
      "    'Olmo': ['ollama/olmo2', 'ollama/olmo2:13b']}\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "describe_models(models=MODEL_LIST, model_to_type=MODEL_TO_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### `check_api_keys`\n",
       "\n",
       "```md\n",
       "\n",
       "    Check if the API keys are set in the environment.\n",
       "\n",
       "    Defaults to the environment variables set in the .env file.\n",
       "\n",
       "    Variables used:\n",
       "    - GEMINI_API_KEY: Google Gemini API key\n",
       "    - OPENAI_API_KEY: OpenAI API key\n",
       "    - ANTHROPIC_API_KEY: Anthropic (Claude) API key\n",
       "    - DEEPSEEK_API_KEY: DeepSeek API key\n",
       "    \n",
       "```\n",
       "----\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### `generate_rhyme_prompt_text`\n",
       "\n",
       "```md\n",
       "\n",
       "    Convenience function for generate_text using rhyme stash.\n",
       "\n",
       "    Args:\n",
       "        args: Arguments for generate_text\n",
       "        stash: Stash to use for caching. Defaults to STASH_GENAI_RHYME_PROMPTS.\n",
       "        verbose: Whether to print verbose output. Defaults to True.\n",
       "        kwargs: Keyword arguments for generate_text\n",
       "\n",
       "    Returns:\n",
       "        str: The generated text\n",
       "    \n",
       "```\n",
       "----\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### `generate_text`\n",
       "\n",
       "```md\n",
       "Generate text with caching support (synchronous interface).\n",
       "    \n",
       "    This is the main text generation function that includes caching capabilities.\n",
       "    Results are cached based on the combination of model, prompt, temperature, and system_prompt.\n",
       "    \n",
       "    Args:\n",
       "        model: The model identifier\n",
       "        prompt: The user prompt/input text\n",
       "        temperature: Sampling temperature for text generation (0.0-1.0)\n",
       "        system_prompt: Optional system prompt/instruction\n",
       "        verbose: If True, print the complete response to stdout\n",
       "        force: If True, bypass cache and force new generation\n",
       "        stash: Cache storage backend for results\n",
       "        \n",
       "    Returns:\n",
       "        str: The complete generated text response (from cache or new generation)\n",
       "    \n",
       "```\n",
       "----\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### `stream_llm`\n",
       "\n",
       "```md\n",
       "Universal streaming interface for language models.\n",
       "    \n",
       "    Automatically routes to the appropriate streaming function based on the model name.\n",
       "    Google models (containing 'gemini') use the Google Generative AI API,\n",
       "    all others use LiteLLM.\n",
       "    \n",
       "    Args:\n",
       "        model: The model identifier\n",
       "        prompt: The user prompt/input text\n",
       "        temperature: Sampling temperature for text generation (0.0-1.0)\n",
       "        system_prompt: Optional system prompt/instruction\n",
       "        verbose: If True, print tokens to stdout as they're generated\n",
       "        \n",
       "    Yields:\n",
       "        str: Individual tokens/text chunks from the model response\n",
       "    \n",
       "```\n",
       "----\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini API key\n",
      "✓ OpenAI API key\n",
      "✓ Anthropic API key\n",
      "✓ DeepSeek API key\n"
     ]
    }
   ],
   "source": [
    "documentation(check_api_keys)\n",
    "documentation(generate_rhyme_prompt_text)\n",
    "documentation(generate_text)\n",
    "documentation(stream_llm)\n",
    "\n",
    "check_api_keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo of prompting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Demo model: claude-3-haiku-20240307\n",
      "* Demo prompt: Write a poem that does NOT rhyme.\n",
      "\n",
      "* Generating text\n",
      "  * model: claude-3-haiku-20240307\n",
      "  * prompt: Write a poem that does NOT rhyme.\n",
      "  * temperature: 0.7\n",
      "  * force: True\n",
      "  * stash: PairtreeHashStash(~/github/generative-formalism/data/stash/genai_rhyme_prompts/pairtree.hashstash.raw+b64/data.db)\n",
      "  * from_cache: False\n",
      "\n",
      "Here is a poem that does not rhyme:\n",
      "\n",
      "Beneath the endless sky,\n",
      "I stand, alone and still.\n",
      "The world around me fades,\n",
      "As I climb the rolling hill.\n",
      "\n",
      "The wind whispers secrets,\n",
      "Caressing my weary face.\n",
      "Time slows to a crawl,\n",
      "In this tranquil, peaceful place.\n",
      "\n",
      "No words can capture this,\n",
      "The beauty that surrounds.\n",
      "Only the soul can feel\n",
      "This sacred, hallowed ground."
     ]
    }
   ],
   "source": [
    "if REPLICATE_LLM_DEMO:\n",
    "    demo_model, demo_prompt = get_demo_model_prompt()\n",
    "    # demo_model, demo_prompt = get_random_model_prompt()  # or a random one\n",
    "    print(f'* Demo model: {demo_model}')\n",
    "    print(f'* Demo prompt: {demo_prompt}\\n')\n",
    "\n",
    "    response_str = generate_rhyme_prompt_text(\n",
    "        model=demo_model,\n",
    "        prompt=demo_prompt,\n",
    "        verbose=True,\n",
    "        force=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
