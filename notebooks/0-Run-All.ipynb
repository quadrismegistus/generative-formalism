{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9a60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### System prompt\n",
       "\n",
       "```md\n",
       "The following is the first 5 lines from a poem given in the user prompt, whose true number of lines is stated there.\n",
       "\n",
       "Complete the poem â€“ do this from memory if you know it; if not, imitate its style and theme for the same number of lines as in the original.\n",
       "\n",
       "Return lines in tab-separated form, starting from line 6 up to the stated number of lines:\n",
       "\n",
       "    line#\tline\n",
       "\n",
       "Do not return any other text besides these tab-separated lines.\n",
       "```\n",
       "\n",
       "----\n",
       "\n",
       "#### User prompt\n",
       "\n",
       "```md\n",
       "NUMBER OF LINES: 15\n",
       "\n",
       "1\tWhile the water tower squats Like a Turkish bird\n",
       "2\ton the ridge and smiles more nicely the other way in green\n",
       "3\tand bougainvillaea to where the warmest houses face north,\n",
       "\n",
       "4\tthe dead well-to-do are ignored and grumble in the plumbing.\n",
       "5\tThis could be a local myth. There could be many.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### `complete_poem`\n",
       "\n",
       "```md\n",
       "Complete a poem using language models with streaming interface.\n",
       "\n",
       "    This function takes the first N lines of a poem and generates the remaining lines\n",
       "    using the specified language model. It uses the streaming LLM interface from llms.py\n",
       "    for efficient text generation with caching support.\n",
       "\n",
       "    Args:\n",
       "        txt (str): The complete original poem text to use as input\n",
       "        first_n_lines (int, optional): Number of initial lines to provide as context.\n",
       "            Defaults to 5.\n",
       "        force (bool, optional): Whether to bypass cache and force new generation.\n",
       "            Defaults to False.\n",
       "        model (str, optional): The model identifier to use for generation.\n",
       "            Defaults to 'deepseek/deepseek-chat'.\n",
       "        verbose (bool, optional): Whether to print verbose output during generation.\n",
       "            Defaults to False.\n",
       "        use_system_prompt (bool, optional): Whether to use a system prompt for the model.\n",
       "            Automatically disabled for text-only models. Defaults to True.\n",
       "        say_poem (bool, optional): Whether to instruct the model to generate a \"poem\"\n",
       "            vs generic \"text\". Affects system prompt. Defaults to True.\n",
       "        temperature (float, optional): Sampling temperature for text generation (0.0-1.0).\n",
       "            Defaults to DEFAULT_TEMPERATURE.\n",
       "        stash (BaseHashStash, optional): Cache storage backend for results.\n",
       "            Defaults to STASH_GENAI if None.\n",
       "        **meta: Additional metadata to include in the user prompt (e.g., AUTHOR, TITLE).\n",
       "\n",
       "    Returns:\n",
       "        pd.DataFrame: DataFrame containing line-by-line completion data with columns:\n",
       "            - stanza_num: Stanza number\n",
       "            - line_num: Line number within the poem\n",
       "            - line_real: Original line from input poem\n",
       "            - line_gen: Generated line from the model\n",
       "            Returns empty DataFrame if generation fails or line count mismatch occurs.\n",
       "\n",
       "    Note:\n",
       "        The function expects the model to return lines in tab-separated format:\n",
       "        \"line_number\tline_text\". If the generated response doesn't match the\n",
       "        expected number of lines, an empty DataFrame is returned.\n",
       "    \n",
       "```\n",
       "----\n",
       "\n",
       "\n",
       "*Call signature*\n",
       "\n",
       "```md\n",
       "complete_poem(\n",
       "    txt\n",
       "    first_n_lines=5\n",
       "    force=False\n",
       "    model='deepseek/deepseek-chat'\n",
       "    verbose=True\n",
       "    use_system_prompt=True\n",
       "    say_poem=True\n",
       "    temperature=0.7\n",
       "    stash=LMDBHashStash(~/github/generative-formalism/data/stash/genai_rhyme_completions/lmdb.hashstash.lz4+b64/data.db)\n",
       "    meta\n",
       ")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Generating text\n",
      "  * model: claude-3-haiku-20240307\n",
      "  * prompt: NUMBER OF LINES: 15  1\tWhile the water tower squats Like a Turkish bird 2\ton the ridge and smiles mo\n",
      "  * temperature: 0.7\n",
      "  * system_prompt: The following is the first 5 lines from a poem given in the user prompt, whose true number of lines \n",
      "  * force: True\n",
      "  * stash: LMDBHashStash(~/github/generative-formalism/data/stash/genai_rhyme_completions/lmdb.hashstash.lz4+b64/data.db)\n",
      "  * from_cache: False\n",
      "\n",
      "6\tThe living have their troubles, their worries and their joys,\n",
      "7\tAnd the dead, they have their own concerns, their own desires.\n",
      "8\tThough they may be ignored, they still have a voice,\n",
      "9\tWhispering through the pipes, echoing in the wires.\n",
      "10\tA local myth, perhaps, but one that holds a grain of truth,\n",
      "11\tFor the dead and the living are bound, in ways unseen.\n",
      "\n",
      "12\tThe water tower, a silent witness to it all,\n",
      "13\tSmiling serenely, its gaze cast both ways.\n",
      "14\tA symbol of the duality of life and death,\n",
      "15\tWhere the living and the dead find their own ways.* Generating text\n",
      "  * model: claude-3-haiku-20240307\n",
      "  * prompt: NUMBER OF LINES: 14  1\tWhen most I wink, then do mine eyes best see, 2\tFor all the day they view thi\n",
      "  * temperature: 0.7\n",
      "  * system_prompt: The following is the first 5 lines from a poem given in the user prompt, whose true number of lines \n",
      "  * force: True\n",
      "  * stash: LMDBHashStash(~/github/generative-formalism/data/stash/genai_rhyme_completions/lmdb.hashstash.lz4+b64/data.db)\n",
      "  * from_cache: False\n",
      "\n",
      "6\tIs my soul's food, and thou art my soul's shade;\n",
      "7\tFor thou art all and all things else are thine,\n",
      "8\tThy light alone leads me to blissful day,\n",
      "9\tWithout thy glorious face that faceless be,\n",
      "10\tThe proudest soul alive needs thee withal.\n",
      "11\tFor being untaught, thy shape invisibly\n",
      "12\tIs to the viewer's eye intoxicate,\n",
      "13\tEnticing most when most unseen, behold,\n",
      "14\tFor all the day they view things unrespected."
     ]
    }
   ],
   "source": [
    "# %run 1-Data-A1-PreprocessingHistoricalCorpora.ipynb\n",
    "# %run 1-Data-A2-SamplingHistoricalCorpora.ipynb\n",
    "# %run 1-Data-B1-PromptingForRhyme.ipynb\n",
    "# %run 1-Data-B2-PromptingForRhymeAtScale.ipynb\n",
    "# %run 1-Data-B3-CollectingRhymePromptingData.ipynb\n",
    "# %run 1-Data-C1-PromptingForCompletions.ipynb\n",
    "# %run 1-Data-C2-PromptingForCompletionsAtScale.ipynb\n",
    "# %run 1-Data-C3-CollectingRhymeCompletionsData.ipynb\n",
    "# %run 2-Methods-A1-MeasuringRhyme.ipynb\n",
    "# %run 2-Methods-A2-MeasuringRhymeInHistoricalCorpus.ipynb\n",
    "# %run 2-Methods-A3-MeasuringRhymeInPromptedPoems.ipynb\n",
    "# %run 2-Methods-A4-MeasuringRhymeInCompletedPoems.ipynb\n",
    "# %run 2-Methods-B1-DetectingMemorizedPoems.ipynb\n",
    "# %run 2-Methods-B2-MeasuringRhymeInMemorizedPoems.ipynb\n",
    "# %run 2-Methods-C1-MeasuringMeter.ipynb\n",
    "# %run 2-Methods-C2-MeasuringMeterInShakespeare.ipynb\n",
    "# %run 2-Methods-C3-MeasuringMeterInSample.ipynb\n",
    "# %run 2-Results-A1-RhymeInHistoricalCorpus.ipynb\n",
    "# %run 2-Results-A2-RhymeInPromptedPoems.ipynb\n",
    "# %run 2-Results-A3-RhymeInCompletedPoems.ipynb\n",
    "# %run 2-Results-A5-RhymeInAllPoems.ipynb\n",
    "# %run 2-Results-B1-RhymeInMemorizedPoems.ipynb\n",
    "# %run 2-Results-C1-StressInSonnets.ipynb\n",
    "# %run 2-Results-C2-IambicPentameterInSonnets.ipynb\n",
    "# %run 2-Results-C3-MetricalSpacerInSonnets.ipynb\n",
    "# %run 8-PlottingPoemMemorization.ipynb\n",
    "# %run 9-TextVsInstructionModels.ipynb\n",
    "# %run 10-Sonnets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2215d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
