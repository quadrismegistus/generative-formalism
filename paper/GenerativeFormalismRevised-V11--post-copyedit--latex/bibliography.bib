@misc{readDrowningSlop2024,
  title = {Drowning in {{Slop}}},
  shorttitle = {Drowning in {{Slop}}},
  author = {Read, Max},
  year = {2024},
  month = sep,
  journal = {New York Magazine: Intelligence},
  urldate = {2025-07-18},
  howpublished = {https://nymag.com/intelligencer/article/ai-generated-content-internet-online-slop-spam.html},
  file = {/Users/rj416/Zotero/storage/8BBKRQRX/ai-generated-content-internet-online-slop-spam.html}
}

@journal{allisonQuantitativeFormatualismExperiment2011,
  title = {Quantitative Formalism: An Experiment},
  author = {Allison, Sarah and Heuser, Ryan and Jockers, Matthew and Moretti, Franco and Witmore, Michael},
  year = {2011},
  month = jan,
  journal = {Stanford Literary Lab Pamphlets},
  number = {1},
  url = {http://litlab.stanford.edu/LiteraryLabPamphlet1.pdf},
  urldate = {2025-01-15}
}


@online{SlopN2Meanings,
  title = {Slop, n.² Meanings, Etymology and More | {{Oxford English Dictionary}}},
  url = {https://www.oed.com/dictionary/slop_n2},
  urldate = {2025-07-19},
  abstract = {slop, n.² meanings, etymology, pronunciation and more in the Oxford English Dictionary},
  langid = {english},
  file = {/Users/ryan/Zotero/storage/DCJNJ5JS/slop_n2.html}
}

@inproceedings{benderDangersStochasticParrots2021a,
  title = {On the Dangers of Stochastic Parrots: {{Can}} Language Models Be Too Big?},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = {2021},
  month = mar,
  publisher = {ACM},
  address = {New York, NY, USA},
  langid = {english}
}


@book{ngaiTheoryGimmickAesthetic2020,
  title = {Theory of the Gimmick: Aesthetic Judgment and Capitalist Form},
  shorttitle = {Theory of the Gimmick},
  author = {Ngai, Sianne},
  date = {2020},
  publisher = {The Belknap Press of Harvard University Press},
  location = {Cambridge [Massachusetts]},
  abstract = {"The gimmick lies latent in every made thing in capitalism, from the banana slicer to the cryptocurrency derivative to the readymade artwork that interprets itself. It includes both the painstakingly devised and the gratuitously disposable. It is what we call things when uncertain if they are over- or underperforming, if they are historically backward or just as problematically advanced, if they are wonders or tricks. With its promises about the saving of time; the reduction of labor; and the expansion of value, the gimmick gives us tantalizing glimpses of a world in which social life will no longer be organized by labor, but also of one regenerating the conditions that keep labor's social necessity in place. As Ngai takes us from the hoaxes of Edgar Allan Poe to artist Stan Douglas's interest in outmoded special effects, we begin to see how the aesthetic manipulation of the gimmick points to a much deeper exploitation. Against academic claims about Marx's value-labor theory's obsolescence, the gimmick reveals it as vibrantly alive in the realm of aesthetics"},
  isbn = {978-0-674-98454-7 978-0-674-27874-5},
  langid = {english}
}

@online{spechtGPTAnswerLife2023,
  title = {42: {{GPT}}'s Answer to {{Life}}, the {{Universe}}, and {{Everything}}},
  author = {Specht, Juan Ignacio},
  date = {2023-10-04},
  url = {https://leniolabs.com/42-gpt-answer-to-life-the-universe-and-everything/},
  urldate = {2025-08-14},
  journaltitle = {Leniolabs},
  langid = {english}
}

@online{kirschenbaumAgainTheory2023,
  title = {Again Theory: A Forum on Language, Meaning, and Intent in the Time of Stochastic Parrots},
  author = {Kirschenbaum, Matthew},
  date = {2023-06-26},
  url = {https://critinq.wordpress.com/2023/06/26/again-theory-a-forum-on-language-meaning-and-intent-in-the-time-of-stochastic-parrots/},
  urldate = {2025-08-14},
  journaltitle = {In the Moment (Critical Inquiry Blog)},
  langid = {english}
}

@article{bassettComputationalTherapeutic2019,
  title = {The Computational Therapeutic: Exploring Weizenbaum's ELIZA as a History of the Present},
  author = {Bassett, Caroline},
  date = {2019},
  journaltitle = {AI \& Society},
  volume = {34},
  number = {4},
  pages = {803--812},
  langid = {english}
}

@inproceedings{heuserHistoricalProsodyMechanical2024,
  title = {Historical Prosody and Mechanical Form},
  author = {Heuser, Ryan and Martin, Meredith},
  date = {2024},
  eventtitle = {ACLA},
  location = {Montreal},
  langid = {english}
}

@inproceedings{walshSonnetNotBot2024,
  title = {Sonnet or {{Not}}, {{Bot}}? {{Poetry Evaluation}} for {{Large Models}} and {{Datasets}}},
  shorttitle = {Sonnet or {{Not}}, {{Bot}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}},
  author = {Walsh, Melanie and Preus, Anna and Antoniak, Maria},
  year = {2024},
  month = oct,
  primaryclass = {cs},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  abstract = {Large language models (LLMs) can now generate and recognize poetry. But what do LLMs really know about poetry? We develop a task to evaluate how well LLMs recognize one aspect of English-language poetry--poetic form--which captures many different poetic features, including rhyme scheme, meter, and word or line repetition. By using a benchmark dataset of over 4.1k human expert-annotated poems, we show that state-of-the-art LLMs can successfully identify both common and uncommon fixed poetic forms--such as sonnets, sestinas, and pantoums--with surprisingly high accuracy. However, performance varies significantly by poetic form; the models struggle to identify unfixed poetic forms, especially those based on topic or visual features. We additionally measure how many poems from our benchmark dataset are present in popular pretraining datasets or memorized by GPT-4, finding that pretraining presence and memorization may improve performance on this task, but results are inconclusive. We release a benchmark evaluation dataset with 1.4k public domain poems and form annotations, results of memorization experiments and data audits, and code.},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/rj416/Zotero/storage/LUKXCX2Y/Walsh et al. - 2024 - Sonnet or Not, Bot Poetry Evaluation for Large Models and Datasets.pdf;/Users/rj416/Zotero/storage/FKLIVZUX/2406.html}
}


@inproceedings{dsouzaChatbotCanonPoetry2023,
  title = {The {{Chatbot}} and the {{Canon}}: {{Poetry Memorization}} in {{LLMs}}},
  author = {D’Souza, Lyra and Mimno, David},
  date = {2023-12-06/2023-12-08},
  location = {Paris, France},
  abstract = {Large language models are able to memorize and generate long passages of text from their pretraining data. Poetry is commonly available on the web and o昀琀en 昀椀ts within language model context sizes. As LLMs continue to grow as a tool in literary analysis, the accessibility of poems will determine the e昀昀ective canon. We assess whether we can prompt current language models to retrieve existing poems, and what methods lead to the most successful retrieval. For the highest performing model, ChatGPT, we then evaluate which features of poets best predict memorization, as well as document changes over time in ChatGPT’s ability and willingness to retrieve poetry.},
  eventtitle = {{{CHR}} 2023: {{Computational Humanities Research Conference}}},
  langid = {english},
  file = {/Users/ryan/Zotero/storage/BW33T85K/D’Souza and Mimno - The Chatbot and the Canon Poetry Memorization in LLMs.pdf}
}

@inproceedings{walshDoesChatGPTHave2024,
  title = {Does ChatGPT Have a Poetic Style?},
  author = {Walsh, Melanie and Preus, Anna and Gronski, Elizabeth},
  date = {2024-10-20},
  extradate = {b},
  eprintclass = {cs},
  location = {Aarhus, Denmark},
  url = {http://arxiv.org/abs/2410.15299},
  urldate = {2024-10-26},
  abstract = {Generating poetry has become a popular application of LLMs, perhaps especially of OpenAI's widely-used chatbot ChatGPT. What kind of poet is ChatGPT? Does ChatGPT have its own poetic style? Can it successfully produce poems in different styles? To answer these questions, we prompt the GPT-3.5 and GPT-4 models to generate English-language poems in 24 different poetic forms and styles, about 40 different subjects, and in response to 3 different writing prompt templates. We then analyze the resulting 5.7k poems, comparing them to a sample of 3.7k poems from the Poetry Foundation and the Academy of American Poets. We find that the GPT models, especially GPT-4, can successfully produce poems in a range of both common and uncommon English-language forms in superficial yet noteworthy ways, such as by producing poems of appropriate lengths for sonnets (14 lines), villanelles (19 lines), and sestinas (39 lines). But the GPT models also exhibit their own distinct stylistic tendencies, both within and outside of these specific forms. Our results show that GPT poetry is much more constrained and uniform than human poetry, showing a strong penchant for rhyme, quatrains (4-line stanzas), iambic meter, first-person plural perspectives (we, us, our), and specific vocabulary like "heart," "embrace," "echo," and "whisper."},
  eventtitle = {{{CHR}} 2024: {{Computational Humanities Research Conference}}},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ryan/Zotero/storage/GFC3WQAY/Walsh et al. - 2024 - Does ChatGPT Have a Poetic Style.pdf}
}


@article{anttilaPhonologicalMetricalVariation2015a,
  title = {Phonological and {{Metrical Variation}} across {{Genres}}},
  author = {Anttila, Arto and Heuser, Ryan},
  date = {2015},
  journaltitle = {Proceedings of the Annual Meetings on Phonology},
  issn = {2377-3324},
  urldate = {2024-11-04},
  abstract = {In this study, we asked whether standard phonological and metrical constraints proposed by phonologists and metricists are useful in identifying arbitrary lines of text as metered verse vs. unmetered prose, focusing on five English-language and five Finnish-language authors. To analyze the dataset we used Prosodic (Heuser, Falk, and Anttila 2010), a software package that provides a phonological analysis and a metrical scansion for raw text. We discovered that phonologically prose and verse differ in the same way in both languages, suggesting that the phonology of genre is universal, whereas metrically the two languages turned out to differ in the expected ways.},
  langid = {english},
  keywords = {automatic scansion,English,Finnish,metrics,Optimality Theory,prosody},
  file = {/Users/ryan/Zotero/storage/8VDEIXDB/Anttila and Heuser - 2015 - Phonological and Metrical Variation across Genres.pdf}
}

@online{heuserProsodic,
  title = {Prosodic},
  author = {Heuser, Ryan},
  url = {https://github.com/quadrismegistus/prosodic},
  urldate = {2024-09-29},
  organization = {Github},
  file = {/Users/ryan/Zotero/storage/77B9AAIX/prosodic.html}
}

@book{emreParaliteraryMakingBad2017a,
  title = {Paraliterary: {{The Making}} of {{Bad Readers}} in {{Postwar America}}},
  shorttitle = {Paraliterary},
  author = {Emre, Merve},
  date = {2017},
  publisher = {University of Chicago Press},
  abstract = {“[Emre’s] intellectual moves . . . are many, subtle, and a pleasure to follow. . . . None of her bad readers could have written this very good book.” —Los Angeles Review of BooksLiterature departments tend to be focused on turning out, “good” readers—attentive to nuance, aware of history, interested in literary texts as self-contained works. But the majority of readers are, to use Merve Emre’s tongue-in-cheek term, “bad” readers. They read fiction and poetry to be moved, distracted, instructed, improved, engaged as citizens. How should we think about those readers, and what should we make of the structures, well outside the academy, that generate them?We should, Emre argues, think of such readers not as non-literary but as paraliterary—thriving outside literary institutions. She traces this phenomenon to the postwar period, when literature played a key role in the rise of American power. At the same time as American universities were producing good readers by the hundreds, many more thousands of bad readers were learning elsewhere to be disciplined public communicators, whether in diplomatic and ambassadorial missions, private and public cultural exchange programs, multinational corporations, or global activist groups. As we grapple with literature’s diminished role in the public sphere, Paraliterary suggests a new way to think about literature, its audience, and its potential, one that looks at the civic institutions that have long engaged readers ignored by the academy.“Paraliterary does for . . . reading . . . what The Program Era did for writing: profoundly upend what we thought we knew about how institutions other than the university have shaped our culture and our engagement with it.” —Deborah Nelson, University of Chicago},
  isbn = {978-0-226-47402-1},
  langid = {english},
  pagetotal = {295},
  keywords = {History / United States / 20th Century,Literary Criticism / American / General,Literary Criticism / Books & Reading,Literary Criticism / Modern / 20th Century}
}

@book{jamesonPoliticalUnconsciousNarrative2002,
  title = {The {{Political Unconscious}}: {{Narrative}} as a {{Socially Symbolic Act}}},
  shorttitle = {The {{Political Unconscious}}},
  author = {Jameson, Fredric},
  date = {2002},
  publisher = {Psychology Press},
  abstract = {In this ground-breaking and influential study Frederic Jameson explores the complex place and function of literature within culture.},
  isbn = {978-0-415-28750-0},
  langid = {english},
  pagetotal = {322},
  keywords = {Language Arts & Disciplines / Rhetoric,Literary Criticism / General,Literary Criticism / Subjects & Themes / Politics,Philosophy / General}
}

@article{morettiConjecturesWorldLiterature2000,
  title = {Conjectures on {{World Literature}}},
  author = {Moretti, Franco},
  date = {2000-02},
  journaltitle = {New Left Review},
  volume = {II},
  number = {1},
  pages = {54--68},
  abstract = {Nearly two hundred years ago, Goethe announced the imminence of a world literature. Here Franco Moretti offers a set of hypotheses for tracking the birth and fate of the novel in the peripheries of Europe, in Latin America, Arab lands, Turkey, China, Japan, West Africa. For the first time, the prospect of a morphology of global letters?}
}

@online{knappHereWavePoem2023,
  title = {Here {{Is}} a {{Wave Poem}} That {{I Wrote}} . . . {{I Hope You Like It}}!},
  author = {Knapp, Steven and Michaels, Walter Benn},
  date = {2023-06-30T16:35:46+00:00},
  url = {https://critinq.wordpress.com/2023/06/30/here-is-a-wave-poem-that-i-wrote-i-hope-you-like-it/},
  urldate = {2024-09-28},
  abstract = {Steven Knapp and Walter Benn Michaels 30 June 2023 In “Against Theory,” after the second wave has washed up and left behind what looks like the second stanza of “A Slumber Did My Spirit Seal,” we a…},
  langid = {english},
  organization = {In the Moment},
  file = {/Users/ryan/Zotero/storage/TDEIHENT/here-is-a-wave-poem-that-i-wrote-i-hope-you-like-it.html}
}

@misc{schoppertBooksUsedTrain2023,
  title = {The Books Used to Train {{LLMs}}},
  author = {Schoppert, Peter},
  date = {2023-03},
  journaltitle = {AI and Copyright},
  url = {https://aicopyright.substack.com/p/the-books-used-to-train-llms},
  abstract = {more on the unseemly use of pirated ebooks to train AIs},
  langid = {english}
}

@book{soRedliningCultureData2020,
  title = {Redlining Culture: A Data History of Racial Inequality and Postwar Fiction},
  shorttitle = {Redlining Culture},
  author = {So, Richard Jean},
  date = {2020},
  publisher = {Columbia University Press},
  location = {New York},
  abstract = {"The canon of postwar American fiction has changed over the past few decades to include far more writers of color. It would appear that we are making progress-recovering marginalized voices and including those who were for far too long ignored. However, is this celebratory narrative borne out in the data? Richard Jean So draws on big data, literary history, and close readings to offer an unprecedented analysis of racial inequality in American publishing that reveals the persistence of an extreme bias toward white authors. In fact, a defining feature of the publishing industry is its vast whiteness, which has denied nonwhite authors, especially black writers, the coveted resources of publishing, reviews, prizes, and sales, with profound effects on the language, form, and content of the postwar novel. Rather than seeing the postwar period as the era of multiculturalism, So argues that we should understand it as the invention of a new form of racial inequality-one that continues to shape the arts and literature today. Interweaving data analysis of large-scale patterns with a consideration of Toni Morrison's career as an editor at Random House and readings of individual works by Octavia Butler, Henry Dumas, Amy Tan, and others, So develops a form of criticism that brings together qualitative and quantitative approaches to the study of literature. A vital and provocative work for American literary studies, critical race studies, and the digital humanities, Redlining Culture shows the importance of data and computational methods for understanding and challenging racial inequality"--},
  isbn = {978-0-231-19772-4 978-0-231-19773-1},
  keywords = {20th century,African American authors History and criticism,American fiction,Authors and publishers,Data processing,Discrimination in employment,History,History and criticism,Literature,Literature and society,Literature publishing,Political aspects History,Race discrimination,United States}
}

@article{underwoodTransformationGenderEnglishLanguage2018,
  title = {The {{Transformation}} of {{Gender}} in {{English-Language Fiction}}},
  author = {Underwood, Ted and Bamman, David and Lee, Sabrina},
  date = {2018-02-13},
  journaltitle = {Journal of Cultural Analytics},
  shortjournal = {Journal of Cultural Analytics},
  volume = {3},
  number = {2},
  pages = {11035},
  doi = {10.22148/16.019},
  url = {https://culturalanalytics.org/article/11035},
  urldate = {2021-09-12},
  abstract = {This essay explores the changing significance of gender in fiction, asking especially whether its prominence in characterization has varied from the end of the eighteenth century to the beginning of the twenty-first. We have reached twoconclusions, which may seem in tension with each other. The first is that gen-der divisions between characters have become less sharply marked over the last 170 years.},
  langid = {english}
}

@article{hansonParametricTheoryPoetic1996,
  title = {A {{Parametric Theory}} of {{Poetic Meter}}},
  author = {Hanson, Kristin and Kiparsky, Paul},
  year = {1996},
  journal = {Language},
  volume = {72},
  pages = {287--335},
  abstract = {Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission. The JSTOR Archive is a trusted digital repository providing for long-term preservation and access to leading academic journals and scholarly literature from around the world. The Archive is supported by libraries, scholarly societies, publishers, and foundations. It is an initiative of JSTOR, a not-for-profit organization with a mission to help the scholarly community take advantage of advances in technology. For more information regarding JSTOR, please contact support@jstor.org.}
}

@misc{porterSpacePoeticMeter2018a,
  title = {The {{Space}} of {{Poetic Meter}}},
  author = {Porter, J.D.},
  year = {2018},
  month = mar,
  journal = {Stanford Literary Lab},
  urldate = {2025-03-15},
  abstract = {The Stanford Literary Lab is a research collective that applies computational criticism, in all its forms, to the study of literature.},
  howpublished = {https://litlab.stanford.edu/hooddistance/},
  langid = {english},
  file = {/Users/rj416/Zotero/storage/E6LUQKTK/hooddistance.html}
}

@book{morettiGraphsMapsTrees2005,
  title = {Graphs, {{Maps}}, {{Trees}}: {{Abstract Models}} for a {{Literary History}}},
  shorttitle = {Graphs, {{Maps}}, {{Trees}}},
  author = {Moretti, Franco},
  year = {2005},
  publisher = {Verso},
  abstract = {Stanford Professor Franco Moretti argues heretically that literature scholars should stop reading books and start counting, graphing and mapping them instead. He insists that such a move could bring new luster to a tired field, one that in some respects, he says, is among the most backwards disciplines in the academy. Literary study, he argues, has been random and unsystematic. For any given periods scholars focus on a select group of a mere few hundred texts: the canon. As a result, they have allowed a narrow distorting slice of history to pass for the total picture. Professor Moretti offers bar charts, maps and time lines instead. His is a history of literature as data points. Charting not only the 18th-century British novel but entire genres - the epistolary, the gothic and the historical novel - as well as the literary output of countries like Japan, Italy, Spain and Nigeria, he shows literary history looks significantly different from what is commonly supposed.},
  googlebooks = {YL2kvMIF8hEC},
  isbn = {978-1-84467-026-0},
  langid = {english},
  keywords = {Literary Criticism / American / General,Literary Criticism / General,Literary Criticism / Semiotics & Theory}
}

@inproceedings{soldainiDolmaOpenCorpus2024,
  title = {Dolma: An {{Open Corpus}} of {{Three Trillion Tokens}} for {{Language Model Pretraining Research}}},
  shorttitle = {Dolma},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Soldaini, Luca and Kinney, Rodney and Bhagia, Akshita and Schwenk, Dustin and Atkinson, David and Authur, Russell and Bogin, Ben and Chandu, Khyathi and Dumas, Jennifer and Elazar, Yanai and Hofmann, Valentin and Jha, Ananya and Kumar, Sachin and Lucy, Li and Lyu, Xinxi and Lambert, Nathan and Magnusson, Ian and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew and Ravichander, Abhilasha and Richardson, Kyle and Shen, Zejiang and Strubell, Emma and Subramani, Nishant and Tafjord, Oyvind and Walsh, Evan and Zettlemoyer, Luke and Smith, Noah and Hajishirzi, Hannaneh and Beltagy, Iz and Groeneveld, Dirk and Dodge, Jesse and Lo, Kyle},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {15725--15788},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  abstract = {Information about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.},
  file = {/Users/rj416/Zotero/storage/D99DYFIE/Soldaini et al. - 2024 - Dolma an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research.pdf}
}
